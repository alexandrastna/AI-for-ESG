{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5JuwUt8SwtlOBy3kACV8L",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexandrastna/AI-for-ESG/blob/main/Notebooks/3_Thesis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thesis 3 - Sentence Extraction from PDF Reports\n",
        "This notebook extracts clean and meaningful sentences from PDF reports (Annual Reports, Sustainability Reports, etc.) using PyMuPDF and spaCy.\n",
        "Each document is processed page by page, removing repetitive headers/footers and excluding index pages.\n",
        "The final output is a CSV file containing all valid sentences, ready for NLP classification."
      ],
      "metadata": {
        "id": "CGbZLx9wLiXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install PyMuPDF and spaCy\n",
        "!pip install pymupdf\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfqk-TGPva-w",
        "outputId": "e32bf84c-e086-468b-b60c-a32849297160"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.26.3-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.26.3\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.9)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 0 ‚Äì Mount Google Drive and install dependencies\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lcP93oJQxoPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 ‚Äì Import libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import fitz  # PyMuPDF\n",
        "import spacy\n",
        "import re\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "W0yJazaSxpe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DE8JIVmvJ3t",
        "outputId": "e191a6ed-fee5-4615-dfc6-c8719acfc77d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 193/202 [28:32<00:21,  2.37s/it]"
          ]
        }
      ],
      "source": [
        "# Step 2 ‚Äì Load SpaCy and the document metadata CSV\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "csv_path = \"/content/drive/MyDrive/Th√®se Master/Data/df_merged_clean.csv\"\n",
        "df_merged = pd.read_csv(csv_path)\n",
        "\n",
        "# Step 3 ‚Äì Dynamic header/footer cleaning function\n",
        "def clean_redundant_headers(text, company, doc_type, year):\n",
        "    company_escaped = re.escape(company)\n",
        "    doc_type_escaped = re.escape(doc_type)\n",
        "\n",
        "    patterns = [\n",
        "        fr\"{company_escaped}\",\n",
        "        fr\"{doc_type_escaped} {year}\",\n",
        "        fr\"{doc_type_escaped} Report {year}\",\n",
        "        fr\"{doc_type_escaped} \\d{{4}}\",\n",
        "        fr\"{year}\",\n",
        "        r\"Page\\s\\d+(\\s+of\\s+\\d+)?\"\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, \"\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 4 ‚Äì Clean sentence extraction function\n",
        "def extract_sentences_from_pdf_clean(path, company, doc_type, year, index_threshold=10):\n",
        "    try:\n",
        "        doc = fitz.open(path)\n",
        "        sentences = []\n",
        "\n",
        "        for page_num, page in enumerate(doc, start=1):\n",
        "            text = page.get_text(\"text\")\n",
        "\n",
        "            # Skip index-heavy pages (table of contents)\n",
        "            pattern_count = len(re.findall(r'\\b\\w+,\\s*\\d+', text))\n",
        "            if pattern_count > index_threshold:\n",
        "                continue\n",
        "\n",
        "            # Clean up repetitive content\n",
        "            text = clean_redundant_headers(text, company, doc_type, year)\n",
        "\n",
        "            # Sentence segmentation\n",
        "            doc_spacy = nlp(text)\n",
        "            for sent in doc_spacy.sents:\n",
        "                s = sent.text.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "                # Noise filters\n",
        "                if (\n",
        "                    len(s) < 30 or\n",
        "                    s.isupper() or\n",
        "                    re.fullmatch(r\"[\\W\\d\\s]+\", s) or\n",
        "                    not s[0].isalpha()\n",
        "                ):\n",
        "                    continue\n",
        "\n",
        "                sentences.append(s)\n",
        "\n",
        "        return sentences\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error parsing {path}: {e}\")\n",
        "        return []\n",
        "\n",
        "# Step 5 ‚Äì Apply extraction to all documents\n",
        "rows = []\n",
        "\n",
        "for idx, row in tqdm(df_merged.iterrows(), total=len(df_merged)):\n",
        "    company = row['Company']\n",
        "    year = row['Year']\n",
        "    doc_type = row['Document Type']\n",
        "    path = row['Path']\n",
        "\n",
        "    if not os.path.isfile(path):\n",
        "        print(f\"‚ö†Ô∏è Fichier non trouv√© : {path}\")\n",
        "        continue\n",
        "\n",
        "    sents = extract_sentences_from_pdf_clean(path, company, doc_type, year)\n",
        "    for sent in sents:\n",
        "        rows.append({\n",
        "            \"company\": company,\n",
        "            \"year\": year,\n",
        "            \"document_type\": doc_type,\n",
        "            \"sentence\": sent\n",
        "        })\n",
        "\n",
        "# Step 6 ‚Äì Export results to Drive\n",
        "df_sentences = pd.DataFrame(rows)\n",
        "output_csv = \"/content/drive/MyDrive/Th√®se Master/Exports2/parsed_sentences.csv\"\n",
        "os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "df_sentences.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"‚úÖ Export termin√© vers : {output_csv}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßπ Post-processing cleanup after manual checks"
      ],
      "metadata": {
        "id": "RQNYj5DLEz0j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the extracted sentences (from Excel)\n",
        "df_sentences = pd.read_excel(\"/content/drive/MyDrive/Th√®se Master/Exports2/parsed_sentences.xlsx\")\n",
        "\n",
        "# 1. Keep only sentences between 10 and 1000 characters\n",
        "df_sentences = df_sentences[df_sentences[\"sentence\"].str.len().between(10, 1000)]\n",
        "\n",
        "# 2. Remove sentences made only of non-alphabetic characters\n",
        "df_sentences = df_sentences[~df_sentences[\"sentence\"].str.contains(r'^[^A-Za-z]*$', na=False)]\n",
        "\n",
        "# 3. Remove numeric-only sentences\n",
        "df_sentences = df_sentences[~df_sentences[\"sentence\"].str.match(r\"^\\d+$\", na=False)]\n",
        "\n",
        "# 4. Remove sentences with more than 15 special characters\n",
        "symbols = set(\"!@#$%^&*()[]{}:;,.?~`+=|\\\\/<>-\")\n",
        "def count_symbols(text):\n",
        "    return sum(1 for char in str(text) if char in symbols)\n",
        "\n",
        "df_sentences = df_sentences[df_sentences[\"sentence\"].apply(count_symbols) <= 15]\n",
        "\n",
        "# Reset index\n",
        "df_sentences = df_sentences.reset_index(drop=True)\n",
        "\n",
        "# Save cleaned CSV\n",
        "df_sentences.to_csv(\"/content/drive/MyDrive/Th√®se Master/Exports2/parsed_sentences.csv\", index=False)\n",
        "print(f\"‚úÖ Nettoyage termin√© : {len(df_sentences)} phrases sauvegard√©es.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O5G7VYREzUl",
        "outputId": "226e5ba3-5c78-41ef-847b-d7772bad6ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Nettoyage termin√© : 201247 phrases sauvegard√©es.\n"
          ]
        }
      ]
    }
  ]
}